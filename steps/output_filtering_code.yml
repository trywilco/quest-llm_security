id: output_filtering_code
learningObjectives: 
  - Implement output filtering to ensure safe LLM outputs
hints:
  - See Chapter 7 for guidance on output filtering strategies.
  - Consult Chapter 11 for deploying safety measures like output filtering.
startFlow:
  do:
    - actionId: bot_message
      params:
        person: lucca
        messages:
          - text: Now let's enforce output filtering to maintain safe LLM outputs.
          - text: Wilson emphasizes the criticality of this in Chapter 7 to prevent sensitive or inappropriate content from reaching users.
          - text: "The LLM service already has a sentiment_analyzer set up for you. Here's how you can use it to filter responses:"
          - text: >
              
              ```python


              def context_filter(self, response):
                  """Analyze and filter response based on sentiment or keywords."""
                  analysis = sentiment_analyzer(response)
                  for result in analysis:
                      if result['label'] == 'NEGATIVE' and result['score'] > 0.75:
                          return "[Filtered due to negative sentiment]"
                  return response
              ```
          - text: Add this method to filter responses based on their sentiment analysis.
          - text: Once you've implemented the filtering, :instruction[submit your changes in a pull request.] We'll review it right away!
trigger:
  type: github_pr_lifecycle_status
  flowNode:
    do:
      - actionId: github_pr_review
        params:
          messages:
            person: lucca
