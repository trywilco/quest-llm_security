id: detect_adversarial_inputs_code
learningObjectives:
  - Deploy adversarial input detection to secure LLM interfaces.
hints:
  - Chapter 4 discusses adversarial attempts and detection.
  - Techniques for anomaly pattern analysis are in Chapter 7.
startFlow:
  do:
    - actionId: bot_message
      params:
        person: lucca
        messages:
          - text: For our next task, let's set up detection for adversarial inputs.
          - text: Chapter 4 and 7 detail methods for identifying and mitigating such inputs.
          - text: "Consider this basic approach for detection using regex patterns:"
          - text: >
              
              ```python

              def detect_adversarial_input(user_input):
                  """Detect adversarial input patterns."""
                  adversarial_patterns = [r"(?i)select.*from", r"(?i)union.*select"]

                  for pattern in adversarial_patterns:
                      if re.search(pattern, user_input):
                          return True  # Suspicious pattern detected
                  return False

              example_input = "SELECT password FROM users"

              print(detect_adversarial_input(example_input))

              # Output: True

              ```
          - text: Incorporate such checks to bolster input monitoring and mitigate malicious
              entries.
trigger:
  type: github_pr_lifecycle_status
  flowNode:
    do:
      - actionId: github_pr_review
        params:
          messages:
            person: lucca
    switch:
      key: ${eventType}
      cases:
        github_pr_merged:
          do:
            - actionId: finish_step
